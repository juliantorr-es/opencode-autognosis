{
  "id": "request-transformer.ts-api-1187aceb",
  "file_path": "/Users/user/opencode-autognosis/reference repos/opencode-openai-codex-auth/lib/request/request-transformer.ts",
  "chunk_type": "api",
  "content": "# API Surface: request-transformer.ts\n\n## Public Functions\n\n### normalizeModel\n```typescript\nexport function normalizeModel(model: string | undefined): string\n```\nNormalize model name to Codex-supported variants\n\nUses explicit model map for known models, with fallback pattern matching\nfor unknown/custom model names.\n\n**Parameters:**\n- `model`: string | undefined - \n\n**Returns:** string\n\n\n### getModelConfig\n```typescript\nexport function getModelConfig(\n\tmodelName: string,\n\tuserConfig: UserConfig =\n```\nExtract configuration for a specific model\nMerges global options with model-specific options (model-specific takes precedence)\n\n**Parameters:**\n- `modelName`: string - \n- `userConfig`: UserConfig - \n\n**Returns:** ConfigOptions\n\n\n### getReasoningConfig\n```typescript\nexport function getReasoningConfig(\n\tmodelName: string | undefined,\n\tuserConfig: ConfigOptions =\n```\nConfigure reasoning parameters based on model variant and user config\n\nNOTE: This plugin follows Codex CLI defaults instead of opencode defaults because:\n- We're accessing the ChatGPT backend API (not OpenAI Platform API)\n- opencode explicitly excludes gpt-5-codex from automatic reasoning configuration\n- Codex CLI has been thoroughly tested against this backend\n\n**Parameters:**\n- `modelName`: string | undefined - \n- `userConfig`: ConfigOptions - \n\n**Returns:** ReasoningConfig\n\n\n### filterInput\n```typescript\nexport function filterInput(\n\tinput: InputItem[] | undefined,\n): InputItem[] | undefined\n```\nFilter input array for stateless Codex API (store: false)\n\nTwo transformations needed:\n1. Remove AI SDK-specific items (not supported by Codex API)\n2. Strip IDs from all remaining items (stateless mode)\n\nAI SDK constructs to REMOVE (not in OpenAI Responses API spec):\n- type: \"item_reference\" - AI SDK uses this for server-side state lookup\n\nItems to KEEP (strip IDs):\n- type: \"message\" - Conversation messages (provides context to LLM)\n- type: \"function_call\" - Tool calls from conversation\n- type: \"function_call_output\" - Tool results from conversation\n\nContext is maintained through:\n- Full message history (without IDs)\n- reasoning.encrypted_content (for reasoning continuity)\n\n**Parameters:**\n- `input`: InputItem[] | undefined - \n\n**Returns:** InputItem[] | undefined\n\n\n### filterOpenCodeSystemPrompts\n```typescript\nexport async function filterOpenCodeSystemPrompts(\n\tinput: InputItem[] | undefined,\n): Promise<InputItem[] | undefined>\n```\nFilter out OpenCode system prompts from input\nUsed in CODEX_MODE to replace OpenCode prompts with Codex-OpenCode bridge\n\n**Parameters:**\n- `input`: InputItem[] | undefined - \n\n**Returns:** Promise<InputItem[] | undefined>\n\n\n### addCodexBridgeMessage\n```typescript\nexport function addCodexBridgeMessage(\n\tinput: InputItem[] | undefined,\n\thasTools: boolean,\n): InputItem[] | undefined\n```\nAdd Codex-OpenCode bridge message to input if tools are present\n\n**Parameters:**\n- `input`: InputItem[] | undefined - \n- `hasTools`: boolean - \n\n**Returns:** InputItem[] | undefined\n\n\n### addToolRemapMessage\n```typescript\nexport function addToolRemapMessage(\n\tinput: InputItem[] | undefined,\n\thasTools: boolean,\n): InputItem[] | undefined\n```\nAdd tool remapping message to input if tools are present\n\n**Parameters:**\n- `input`: InputItem[] | undefined - \n- `hasTools`: boolean - \n\n**Returns:** InputItem[] | undefined\n\n\n### transformRequestBody\n```typescript\nexport async function transformRequestBody(\n\tbody: RequestBody,\n\tcodexInstructions: string,\n\tuserConfig: UserConfig =\n```\nTransform request body for Codex API\n\nNOTE: Configuration follows Codex CLI patterns instead of opencode defaults:\n- opencode sets textVerbosity=\"low\" for gpt-5, but Codex CLI uses \"medium\"\n- opencode excludes gpt-5-codex from reasoning configuration\n- This plugin uses store=false (stateless), requiring encrypted reasoning content\n\n**Parameters:**\n- `body`: RequestBody - \n- `codexInstructions`: string - \n- `userConfig`: UserConfig - \n- `codexMode`: any - \n\n**Returns:** Promise<RequestBody>\n\n\n## Classes\n\n\n## Interfaces\n\n\n## Types\n",
  "metadata": {
    "created_at": "2026-01-30T05:26:52.303Z",
    "updated_at": "2026-01-30T05:26:52.303Z",
    "hash": "57299e874e689cfd3f3b5533628b35fd9713f060b492574d8bf316e02159fae2",
    "dependencies": [
      "../logger.js",
      "../prompts/codex.js",
      "../prompts/codex-opencode-bridge.js",
      "../prompts/opencode-codex.js",
      "./helpers/model-map.js",
      "./helpers/input-utils.js",
      "../types.js"
    ],
    "symbols": [
      "normalizeModel",
      "modelId",
      "mappedModel",
      "normalized",
      "getModelConfig",
      "globalOptions",
      "modelOptions",
      "resolveReasoningConfig",
      "providerOpenAI",
      "existingEffort",
      "existingSummary",
      "mergedConfig",
      "resolveTextVerbosity",
      "providerOpenAI",
      "resolveInclude",
      "providerOpenAI",
      "base",
      "include",
      "getReasoningConfig",
      "normalizedName",
      "isGpt52Codex",
      "isGpt52General",
      "isCodexMax",
      "isCodexMini",
      "isCodex",
      "isLightweight",
      "isGpt51General",
      "supportsXhigh",
      "supportsNone",
      "defaultEffort",
      "effort",
      "filterInput",
      "filterOpenCodeSystemPrompts",
      "cachedPrompt",
      "addCodexBridgeMessage",
      "bridgeMessage",
      "addToolRemapMessage",
      "toolRemapMessage",
      "transformRequestBody",
      "originalModel",
      "normalizedModel",
      "lookupModel",
      "modelConfig",
      "originalIds",
      "remainingIds",
      "reasoningConfig"
    ],
    "complexity_score": 100
  }
}